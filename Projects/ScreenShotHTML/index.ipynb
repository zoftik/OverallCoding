{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to replace 'path_to_web_driver_executable', 'username_input_id', 'password_input_id', 'login_button_id', 'filter_element_1_id', 'filter_element_2_id', 'criteria_element_1_id', 'criteria_element_2_id', 'filter_criteria_1', 'filter_criteria_2', and adjust the list of urls as needed for the specific website and pages you want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the path to the web driver executable (e.g., ChromeDriver)\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='c:/Windows/chromedriver_win64/chromedriver.exe')\n",
    "\n",
    "\n",
    "# Define your login credentials\n",
    "username = 'your_username'\n",
    "password = 'your_password'\n",
    "\n",
    "# Open the login page and log in (replace with your login logic)\n",
    "driver.get('https://example.com/login')\n",
    "username_field = driver.find_element(By.ID, 'username_input_id')  # Replace with actual username input element\n",
    "password_field = driver.find_element(By.ID, 'password_input_id')  # Replace with actual password input element\n",
    "login_button = driver.find_element(By.ID, 'login_button_id')  # Replace with actual login button element\n",
    "\n",
    "username_field.send_keys(username)\n",
    "password_field.send_keys(password)\n",
    "login_button.click()\n",
    "\n",
    "# Define a list of URLs you want to process\n",
    "urls = ['https://example.com/page1', 'https://example.com/page2', 'https://example.com/page3']\n",
    "\n",
    "# Loop through the URLs\n",
    "for url in urls:\n",
    "    driver.get(url)  # Open the URL\n",
    "    \n",
    "    # Add filters (replace with actual filter interaction logic)\n",
    "    filter_element_1 = driver.find_element(By.ID, 'filter_element_1_id')  # Replace with actual filter elements\n",
    "    filter_element_2 = driver.find_element(By.ID, 'filter_element_2_id')  # Replace with actual filter elements\n",
    "\n",
    "    filter_element_1.click()  # Click on the first filter element\n",
    "    # Add filter criteria (replace with actual filter criteria selection)\n",
    "    criteria_element_1 = driver.find_element(By.ID, 'criteria_element_1_id')  # Replace with actual criteria elements\n",
    "    criteria_element_1.send_keys('filter_criteria_1')  # Input the filter criteria\n",
    "    criteria_element_1.send_keys(Keys.RETURN)  # Apply the filter\n",
    "    \n",
    "    filter_element_2.click()  # Click on the second filter element\n",
    "    # Add filter criteria (replace with actual filter criteria selection)\n",
    "    criteria_element_2 = driver.find_element(By.ID, 'criteria_element_2_id')  # Replace with actual criteria elements\n",
    "    criteria_element_2.send_keys('filter_criteria_2')  # Input the filter criteria\n",
    "    criteria_element_2.send_keys(Keys.RETURN)  # Apply the filter\n",
    "\n",
    "    # Wait for 3 seconds to allow the data to load (you can adjust the wait time)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Capture the filtered data from the web page (replace with actual data scraping logic)\n",
    "    filtered_data = driver.find_elements(By.CLASS_NAME, 'data-row')  # Replace with actual data elements\n",
    "\n",
    "    # Convert the captured data to a list or DataFrame (replace with your logic)\n",
    "    data_list = [data.text for data in filtered_data]\n",
    "\n",
    "    # Create a DataFrame (replace with your data structure)\n",
    "    df = pd.DataFrame(data_list, columns=['Column1', 'Column2', 'Column3'])  # Adjust column names as needed\n",
    "\n",
    "    # Create an Excel workbook and save the DataFrame to it\n",
    "    excel_file = f'{url.replace(\"https://\", \"\").replace(\".\", \"_\")}_filtered_data.xlsx'\n",
    "    df.to_excel(excel_file, index=False, engine='openpyxl')\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zoftik\\AppData\\Local\\Temp\\ipykernel_43548\\2416381872.py:7: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True  # Run Chrome in headless mode, no GUI\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\VSCode\\Projects\\ScreenShotHTML\\index.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCSV file created as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39moutput_csv\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in same path.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     main()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Now you can work with the parsed HTML (e.g., extract product codes)\u001b[39;00m\n",
      "\u001b[1;32md:\\VSCode\\Projects\\ScreenShotHTML\\index.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Parse command-line arguments\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     parser \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mArgumentParser(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mScrape and process data from an HTML file.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39mhtml_file\u001b[39m\u001b[39m'\u001b[39m, help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput HTML file name\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Projects/ScreenShotHTML/index.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39moutput_csv\u001b[39m\u001b[39m'\u001b[39m, help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOutput CSV file name\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up a headless browser\n",
    "options = Options()\n",
    "options.headless = True  # Run Chrome in headless mode, no GUI\n",
    "\n",
    "# Provide the path to your Chrome webdriver\n",
    "driver_path = './chromedriver.exe'\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.newlook.com/row/womens'\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Get the page source\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "def scrape_data(html_filename):\n",
    "    # Parse the HTML file\n",
    "    soup = BeautifulSoup(open(html_filename), 'html.parser')\n",
    "\n",
    "    # Scraping logic (your find_all_wrapper function)\n",
    "    a = []\n",
    "    for li in soup.find_all(class_=\"plp-item\"):\n",
    "        a.append(li.a.get('href'))\n",
    "    return a\n",
    "\n",
    "\n",
    "def process_data(input_list):\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Process the data\n",
    "    for item in input_list:\n",
    "        last_9 = item[-9:]\n",
    "        data.append([item, last_9])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Scrape and process data from an HTML file.')\n",
    "    parser.add_argument('html_file', help='Input HTML file name')\n",
    "    parser.add_argument('output_csv', help='Output CSV file name')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Scrape the data\n",
    "    input_list = scrape_data(args.html_file)\n",
    "\n",
    "    # Process the data\n",
    "    data = process_data(input_list)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Original String\", \"Last 9 Digits\"])\n",
    "\n",
    "    # Add the prefix\n",
    "    prefix = 'https://www.newlook.com'\n",
    "    df['Original String'] = df['Original String'].apply(lambda x: prefix + x)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(args.output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file created as '{args.output_csv}' in same path.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Now you can work with the parsed HTML (e.g., extract product codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
