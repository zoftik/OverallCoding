{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1' , attrs={'class':\"product-description__name\"}) \n",
    "        \n",
    "        title_value = title.text\n",
    "        \n",
    "        title_string = title_value.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    \n",
    "    return title_string\n",
    "\n",
    "\n",
    "def get_Product_code(soup):\n",
    "    try :\n",
    "        product_code = soup.find('div' , attrs={'class':\"product-details--product-code\"}).text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        product_code = \"\"\n",
    "        \n",
    "    return product_code\n",
    "\n",
    "\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('span' , attrs={'class':\"product-description__price\"}).string.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "        \n",
    "    return price\n",
    "            \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    headers = ({'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'})\n",
    "    # The webpage URL\n",
    "    urls = 'https://www.newlook.com/row/mens/clothing/c/row-mens-clothing#/?q=:relevance&page=0&sort=relevance&content=false'\n",
    "\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(urls, headers=headers)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all('a' , attrs= {'class':'plp-carousel__img-link'})\n",
    "\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "    d = {\"title\":[], \"price\":[], \"product_code\":[]}\n",
    "    \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    "        new_webpage = requests.get(\"https://www.newlook.com\" + link, headers=headers)\n",
    "\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to display all necessary product information\n",
    "        d['title'].append(get_title(new_soup))\n",
    "        d['price'].append(get_price(new_soup))\n",
    "        d['product_code'].append(get_Product_code(new_soup))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Define a function to scrape product details from a page\n",
    "def scrape_page(url):\n",
    "    webpage = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all('a', attrs={'class': 'plp-carousel__img-link'})\n",
    "\n",
    "    for link in links:\n",
    "        new_webpage = requests.get(\"https://www.newlook.com\" + link.get('href'), headers=headers)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        title = get_title(new_soup)\n",
    "        price = get_price(new_soup)\n",
    "        product_code = get_Product_code(new_soup)\n",
    "\n",
    "        d['title'].append(title)\n",
    "        d['price'].append(price)\n",
    "        d['product_code'].append(product_code)\n",
    "\n",
    "# The initial URL\n",
    "base_url = urls\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    current_page_url = f'{base_url}?page={page_number}'\n",
    "\n",
    "    # Scrape product details from the current page\n",
    "    scrape_page(current_page_url)\n",
    "\n",
    "    # Check if there is a next page by inspecting the website's structure\n",
    "    # If there's no next page, break the loop\n",
    "    next_button = soup.find('span', class_='ng-scope')  # Example class name for the \"Next\" button\n",
    "\n",
    "    if next_button:\n",
    "        # There is a next page, so continue to the next iteration\n",
    "        page_number += 1\n",
    "    else:\n",
    "        # No \"Next\" button found, break the loop\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "    # Increment the page number for the next iteration\n",
    "    page_number += 1\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "newlook_df = pd.DataFrame.from_dict(d)\n",
    "newlook_df['title'].replace('', np.nan, inplace=True)\n",
    "newlook_df = newlook_df.dropna(subset=['title'])\n",
    "newlook_df.to_csv(\"newlook_data.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define functions to extract product details\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1', attrs={'class': \"product-description__name\"})\n",
    "        title_value = title.text\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    return title_string\n",
    "\n",
    "def get_Product_code(soup):\n",
    "    try:\n",
    "        product_code = soup.find('div', attrs={'class': \"product-details--product-code\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        product_code = \"\"\n",
    "    return product_code\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('span', attrs={'class': \"product-description__price\"}).string.strip()\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "    return price\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.69'}\n",
    "    base_url = 'https://www.newlook.com/row/womens/clothing/dresses/c/row-womens-clothing-dresses#/?q=:relevance&page={page_number}&sort=relevance&content=false'\n",
    "    page_number = 0\n",
    "    \n",
    "    final_data = {\"title\": [], \"price\": [], \"product_code\": []}\n",
    "    \n",
    "    while True:\n",
    "        current_page_url = f'{base_url}?page={page_number}'\n",
    "        webpage = requests.get(current_page_url, headers=headers)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        links = soup.find_all('a', attrs={'class': 'plp-carousel__img-link'})\n",
    "        \n",
    "        if not links:\n",
    "            # No more products on this page, break the loop\n",
    "            break\n",
    "        \n",
    "        for link in links:\n",
    "            product_url = \"https://www.newlook.com\" + link.get('href')\n",
    "            product_webpage = requests.get(product_url, headers=headers)\n",
    "            product_soup = BeautifulSoup(product_webpage.content, \"html.parser\")\n",
    "            \n",
    "            title = get_title(product_soup)\n",
    "            price = get_price(product_soup)\n",
    "            product_code = get_Product_code(product_soup)\n",
    "            \n",
    "            final_data['title'].append(title)\n",
    "            final_data['price'].append(price)\n",
    "            final_data['product_code'].append(product_code)\n",
    "        \n",
    "        page_number += 1\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    newlook_df = pd.DataFrame.from_dict(final_data)\n",
    "    newlook_df['title'].replace('', np.nan, inplace=True)\n",
    "    newlook_df = newlook_df.dropna(subset=['title'])\n",
    "    newlook_df.to_csv(\"newlook_data.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "import httpx\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class product:\n",
    "    product_code: str\n",
    "    price: str\n",
    "    title: str\n",
    "\n",
    "\n",
    "def get_html(page):\n",
    "    url = f'https://www.newlook.com/row/womens/clothing/c/row-womens-clothing#/?q=:relevance&page={page}&sort=relevance&content=false'\n",
    "    resp = httpx.get(url)\n",
    "    return HTMLParser(resp.text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = get_html(1)\n",
    "    print(html)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define functions to extract product details\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1', attrs={'class': \"product-description__name\"})\n",
    "        title_value = title.text\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    return title_string\n",
    "\n",
    "def get_Product_code(soup):\n",
    "    try:\n",
    "        product_code = soup.find('div', attrs={'class': \"product-details--product-code\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        product_code = \"\"\n",
    "    return product_code\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('span', attrs={'class': \"product-description__price\"}).string.strip()\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "    return price\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.69'}\n",
    "    base_url = 'https://www.newlook.com/row/womens/clothing/dresses/c/row-womens-clothing-dresses#/?q=:relevance&page={page_number}&sort=relevance&content=false'\n",
    "    page_number = 1\n",
    "    \n",
    "    final_data = {\"title\": [], \"price\": [], \"product_code\": []}\n",
    "    \n",
    "    while True:\n",
    "        current_page_url = f'{base_url}?page={page_number}'\n",
    "        webpage = requests.get(current_page_url, headers=headers)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        links = soup.find_all('a', attrs={'class': 'plp-carousel__img-link'})\n",
    "        \n",
    "        if not links:\n",
    "            # No more products on this page, break the loop\n",
    "            break\n",
    "        \n",
    "        for link in links:\n",
    "            product_url = \"https://www.newlook.com\" + link.get('href')\n",
    "            product_webpage = requests.get(product_url, headers=headers)\n",
    "            product_soup = BeautifulSoup(product_webpage.content, \"html.parser\")\n",
    "            \n",
    "            title = get_title(product_soup)\n",
    "            price = get_price(product_soup)\n",
    "            product_code = get_Product_code(product_soup)\n",
    "            \n",
    "            final_data['title'].append(title)\n",
    "            final_data['price'].append(price)\n",
    "            final_data['product_code'].append(product_code)\n",
    "        \n",
    "        # Check for the absence of a \"Next Page\" button\n",
    "        next_button = soup.find('a', attrs={'class': 'next-page'})\n",
    "        if not next_button:\n",
    "            # No more next page, break the loop\n",
    "            break\n",
    "        \n",
    "        page_number += 1\n",
    "        \n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    newlook_df = pd.DataFrame.from_dict(final_data)\n",
    "    newlook_df['title'].replace('', np.nan, inplace=True)\n",
    "    newlook_df = newlook_df.dropna(subset=['title'])\n",
    "    newlook_df.to_csv(\"newlook_data.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Inside your loop:\n",
    "\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('h1' , attrs={'class':\"product-description__name\"}) \n",
    "        \n",
    "        title_value = title.text\n",
    "        \n",
    "        title_string = title_value.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    \n",
    "    return title_string\n",
    "\n",
    "\n",
    "def get_Product_code(soup):\n",
    "    try :\n",
    "        product_code = soup.find('div' , attrs={'class':\"product-details--product-code\"}).text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        product_code = \"\"\n",
    "        \n",
    "    return product_code\n",
    "\n",
    "\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('span' , attrs={'class':\"product-description__price\"}).string.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "        \n",
    "    return price\n",
    "            \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    headers = ({'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'})\n",
    "    # The webpage URL\n",
    "    urls = 'https://www.newlook.com/row/boys/clothing/c/row-boys-clothing#/?q=:relevance&page=1&sort=relevance&content=false'\n",
    "\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(urls, headers=headers)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all('a' , attrs= {'class':'plp-carousel__img-link'})\n",
    "\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "    d = {\"title\":[], \"price\":[], \"product_code\":[]}\n",
    "    \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    "        new_webpage = requests.get(\"https://www.newlook.com\" + link, headers=headers)\n",
    "\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to display all necessary product information\n",
    "        d['title'].append(get_title(new_soup))\n",
    "        d['price'].append(get_price(new_soup))\n",
    "        d['product_code'].append(get_Product_code(new_soup))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Define a function to scrape product details from a page\n",
    "def scrape_page(url):\n",
    "    webpage = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all('a', attrs={'class': 'plp-carousel__img-link'})\n",
    "\n",
    "    for link in links:\n",
    "        new_webpage = requests.get(\"https://www.newlook.com\" + link.get('href'), headers=headers)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        title = get_title(new_soup)\n",
    "        price = get_price(new_soup)\n",
    "        product_code = get_Product_code(new_soup)\n",
    "\n",
    "        d['title'].append(title)\n",
    "        d['price'].append(price)\n",
    "        d['product_code'].append(product_code)\n",
    "\n",
    "\n",
    "# The initial URL\n",
    "base_url = urls\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    \n",
    "    current_page_url = f'{base_url}?page={page_number}'\n",
    "\n",
    "    # Scrape product details from the current page\n",
    "    scrape_page(current_page_url)\n",
    "    time.sleep(2) \n",
    "\n",
    "    # Check if there is a next page by inspecting the website's structure\n",
    "    # If there's no next page, break the loop\n",
    "    next_button = soup.find('span', class_='ng-scope')  # Example class name for the \"Next\" button\n",
    "\n",
    "    if next_button:\n",
    "        # There is a next page, so continue to the next iteration\n",
    "        page_number += 1\n",
    "    else:\n",
    "        # No \"Next\" button found, break the loop\n",
    "        break\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "newlook_df = pd.DataFrame.from_dict(d)\n",
    "newlook_df['title'].replace('', np.nan, inplace=True)\n",
    "newlook_df = newlook_df.dropna(subset=['title'])\n",
    "newlook_df.to_csv(\"newlook_data.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
