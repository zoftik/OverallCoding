{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file Created in current folder as 'Output.csv'.\n"
     ]
    }
   ],
   "source": [
    "### Importing Necessary Lib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### Giving Input At HTML File\n",
    "\n",
    "html_doc = ('index.html')\n",
    "# Parse the html file\n",
    "soup = BeautifulSoup(open(html_doc), 'html.parser') \n",
    "\n",
    "#### Creating Function To get Desired Data\n",
    "\n",
    "def find_all_wrapper():\n",
    "    \n",
    "    a = []\n",
    "    for li in soup.find_all(class_=\"plp-item\"):\n",
    "        a.append(li.a.get('href'))\n",
    "    return a\n",
    "\n",
    "\n",
    "input_list = find_all_wrapper()\n",
    "\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Extract the last 8 digits and populate the data list\n",
    "for item in input_list:\n",
    "    last_9 = int(item[-9:])\n",
    "    data.append([item, last_9])\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data, columns=[\"Original String\", \"Last 9 Digits\"])\n",
    "\n",
    "\n",
    "\n",
    "prefix = 'https://www.newlook.com'\n",
    "df['Original String'] = df['Original String'].apply(lambda x: prefix + x)\n",
    "\n",
    "# Define the CSV filename\n",
    "csv_filename = \"Output.csv\"\n",
    "\n",
    "# Export the DataFrame to a single CSV file\n",
    "df.to_csv(csv_filename)\n",
    "\n",
    "print(f\"CSV file Created in current folder as '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] html_file output_csv\n",
      "ipykernel_launcher.py: error: the following arguments are required: html_file, output_csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(html_filename):\n",
    "    # Parse the HTML file\n",
    "    soup = BeautifulSoup(open(html_filename), 'html.parser')\n",
    "\n",
    "    # Scraping logic (your find_all_wrapper function)\n",
    "    a = []\n",
    "    for li in soup.find_all(class_=\"plp-item\"):\n",
    "        a.append(li.a.get('href'))\n",
    "    return a\n",
    "\n",
    "def process_data(input_list):\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Process the data\n",
    "    for item in input_list:\n",
    "        last_9 = item[-9:]\n",
    "        data.append([item, last_9])\n",
    "\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser(description='Scrape and process data from an HTML file.')\n",
    "    parser.add_argument('html_file', help='Input HTML file name')\n",
    "    parser.add_argument('output_csv', help='Output CSV file name')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Scrape the data\n",
    "    input_list = scrape_data(args.html_file)\n",
    "\n",
    "    # Process the data\n",
    "    data = process_data(input_list)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Original String\", \"Last 9 Digits\"])\n",
    "\n",
    "    # Add the prefix\n",
    "    prefix = 'https://www.newlook.com'\n",
    "    df['Original String'] = df['Original String'].apply(lambda x: prefix + x)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(args.output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file created as '{args.output_csv}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] html_file output_csv\n",
      "ipykernel_launcher.py: error: the following arguments are required: html_file, output_csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_data(html_filename):\n",
    "    # Parse the HTML file\n",
    "    soup = BeautifulSoup(open(html_filename), 'html.parser')\n",
    "\n",
    "    # Scraping logic (your find_all_wrapper function)\n",
    "    a = []\n",
    "    for li in soup.find_all(class_=\"plp-item\"):\n",
    "        a.append(li.a.get('href'))\n",
    "    return a\n",
    "\n",
    "\n",
    "def process_data(input_list):\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Process the data\n",
    "    for item in input_list:\n",
    "        last_9 = item[-9:]\n",
    "        data.append([item, last_9])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Scrape and process data from an HTML file.')\n",
    "    parser.add_argument('html_file', help='Input HTML file name')\n",
    "    parser.add_argument('output_csv', help='Output CSV file name')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Scrape the data\n",
    "    input_list = scrape_data(args.html_file)\n",
    "\n",
    "    # Process the data\n",
    "    data = process_data(input_list)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Original String\", \"Last 9 Digits\"])\n",
    "\n",
    "    # Add the prefix\n",
    "    prefix = 'https://www.newlook.com'\n",
    "    df['Original String'] = df['Original String'].apply(lambda x: prefix + x)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(args.output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file created as '{args.output_csv}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
