{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index for Search\n",
    "# 1.import pandas\n",
    "# 2.to read and export a excel/csv file--also given to export a excel/csv file of a pivot table\n",
    "# 3.to check total no. of rows & colums in dataframe\n",
    "# 4.View a particular column\n",
    "# 5.delete a column/delete columns\n",
    "# 6.value counts of a column\n",
    "# 7.Rename Columns of Data Frame\n",
    "# 8.to find first four /second and third/last two digits from say number of 10 digits\n",
    "# 9.Rename Columns of Data Frame\n",
    "# 10.Selecting Columns or Rows/Filtering Records\n",
    "# 11.Exclusion\n",
    "# 12.Creating/adding New Columns\n",
    "# 13.python code of alternate of if function in excel--- np.where\n",
    "# 14.Handling Missing Values\n",
    "# 15.sorting\n",
    "# 16.indexing\n",
    "# 17.giving names to columns after indexing/transposing\n",
    "# 18.Aggregate--groupby/cummulative count/pivot\n",
    "# 19.simple cross tabulation of two factors\n",
    "# 20.to check data type\n",
    "# 21.to merge two tables\n",
    "# 22.to strip\n",
    "# 23.Identify missing values of dataframe\n",
    "# 24.to transpose data\n",
    "# 25.append\n",
    "# 26.to concat\n",
    "# 27.to check data type\n",
    "# 28.identify unique values\n",
    "# 29.How to remove duplicate values of a variable\n",
    "# 30.Applying function to element, column or dataframe\n",
    "# 31.Basic Stats\n",
    "# 32.How to generate frequency tables with pandas\n",
    "# 33.Convert numeric variables to string variables and vice versa\n",
    "# 34.random sampling in pandas\n",
    "# 35.how to write or function/how to write and function\n",
    "# 36.round off\n",
    "# 37.how to replace a value by other\n",
    "# 38.how to convert a column to string/text\n",
    "# 39.how to take values(text & number) of different columns of a same customer in a single column\n",
    "# 40.#how to drop a column/multiple column\n",
    "# 41.Data pulling from MYSQL- Pulling scrip level data\n",
    "# 42.Data pulling from MYSQL- dynamically pulling out last 15 months’ data\n",
    "# 43.Data pulling from MYSQL- Pulling trade level data- cash & derivatives\n",
    "# 44.How to read from a sheet of excel/csv\n",
    "# 45.Write different worksheets in same excel\n",
    "# 46.nested if in python\n",
    "# 47.selecting only one column from one dataframe and put it to another dataframe\n",
    "# 48.remove duplicates\n",
    "# 49.Indexing, Slicing and Subsetting DataFrames in Python\n",
    "# 50.Finding location/row number by giving a particular column value\n",
    "# 51.Finding all details by giving a particular column value(also works if the value comes multiple times)\n",
    "# 52.Convert string to numeric\n",
    "# 53.Date time convert/today\n",
    "# 54.remove missing rows\n",
    "# 55.Only take selected columns from data frame\n",
    "# 56.How to do round\n",
    "# 57.How to concatenate\n",
    "# 58.Line break/transpose/concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to read a excel/csv file\n",
    "pd.read_excel(\"path of the file.xlsx\")\n",
    "pd.read_csv(\"path of the file.csv\")\n",
    "\n",
    "#to export a excel/csv file\n",
    "df.to_excel(\"path of the file.xlsx\")\n",
    "df.to_csv(\"path of the file.csv\")\n",
    "#to export a excel/csv file of a pivot table\n",
    "df5.to_excel(\"path.xlsx\", merge_cells=False)\n",
    "df5.to_csv(\"path.csv\", merge_cells=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to check total no. of rows & colums in dataframe\n",
    "df.shape\n",
    "#to see first five data points\n",
    "df.head()   #here df is dataframe\n",
    "#to see last five data points\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#View a particular column\n",
    "df.columnname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete a column\n",
    "del df['columnname']\n",
    "#delete columns\n",
    "t_df1.drop(['column1','column2'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#value counts of a column #to count say how many profit or loss from a column named profit/loss\n",
    "df.columnname.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to find first four digits from say number of 10 digits\n",
    "df['newcolumn name'] = df['column name'].str[:4]\n",
    "#to find second and third digits from say number of 10 digits\n",
    "df['newcolumn name'] = df['column name'].str[2:4]\n",
    "#to find last two digits from say number of 10 digits\n",
    "df['newcolumn name'] = df['column name'].str[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rename Columns of Data Frame\n",
    "\n",
    "#Rename method helps to rename column of data frame.\n",
    "df2=df.rename(columns={‘old_columnname’:’new_columnname’}) \n",
    "#This statement will create a new data frame with new column name. \n",
    "\n",
    "#To rename the column of existing data frame, set inplace=True\n",
    "df.rename(columns={‘old_columnname’:’new_columnname’}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting Columns or Rows\n",
    "\n",
    "# Accessing sub data frames\n",
    "df[[‘column1’,’column2’]]\n",
    "\n",
    "#Filtering Records\n",
    "df = df[ df[‘column1’]>10]\n",
    "df[ (df[‘column1’]>10) & df[‘column2’]==30]\n",
    "df[ (df[‘column1’]>10) | df[‘column2’]==30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exclusion\n",
    "import pandas as pd\n",
    "import win32com.client as wd\n",
    "conn = wd.Dispatch(r'ADODB.Connection')\n",
    "conn.CommandTimeout = 3600\n",
    "CONNECTIONSTRING = \"Provider=MSOLAP.5;Integrated Security=SSPI;Persist Security Info=True;Initial Catalog=ODS;Data Source=db71.kotakseconline.com;MDX Compatibility=1;Safety Options=2;MDX Missing Member Mode=Error\"\n",
    "conn.Open(CONNECTIONSTRING)\n",
    "rs = wd.Dispatch(r'ADODB.Recordset')\n",
    "\n",
    "\n",
    "# mdx_query3 = \"Select {[Measures].[DP Valuation]} on Columns,{[Client Code].children} DIMENSION PROPERTIES MEMBER_CAPTION,[Client Code].[Branch Franchisee],[Client Code].[Cash Delivery Cost],[Client Code].[Cash Squareup Cost],[Client Code].[Cc Name],[Client Code].[Channel],[Client Code].[Client],[Client Code].[Client Active Status],[Client Code].[Client Category],[Client Code].[Client Introduction Date],[Client Code].[Client Name],[Client Code].[Client Type],[Client Code].[Dealer Code],[Client Code].[Dealer Name],[Client Code].[Dealer2 Code],[Client Code].[Dealer2 Name],[Client Code].[Dealing All India Vertical Head Code],[Client Code].[Dealing All India Vertical Head Name],[Client Code].[Dealing Area Manager Code],[Client Code].[Dealing Area Manager Name],[Client Code].[Dealing Branch],[Client Code].[Dealing Business Head Code],[Client Code].[Dealing Business Head Name],[Client Code].[Dealing City],[Client Code].[Dealing Regional Manager Code],[Client Code].[Dealing Regional Manager Name],[Client Code].[Dealing State],[Client Code].[Dealing Sublocation],[Client Code].[Dealing Team Manager Code],[Client Code].[Dealing Team Manager Name],[Client Code].[Dealing Territory Manager Code],[Client Code].[Dealing Territory Manager Name],[Client Code].[Dealing Zone],[Client Code].[Derv Future Expiry Cost],[Client Code].[Derv Future Settl Squareup Cost],[Client Code].[Derv Future Squareup Cost],[Client Code].[Derv Options Exercise Assigned Cost],[Client Code].[Derv Options Lot Exercise Assigned Cost],[Client Code].[Derv Options Lot Settl Squareup Cost],[Client Code].[Derv Options Lot Squareup Cost],[Client Code].[Derv Options Settl Squareup Cost],[Client Code].[Derv Options Squareup Cost],[Client Code].[DP Client Id],[Client Code].[DP NAME],[Client Code].[Franchisee Main Code],[Client Code].[Franchisee Name],[Client Code].[Introduction Month],[Client Code].[Introduction Quarter],[Client Code].[Introduction Year],[Client Code].[Last Trade Date],[Client Code].[LOB],[Client Code].[LOC],[Client Code].[LOC code],[Client Code].[Location Code],[Client Code].[Location-Type Of Client],[Client Code].[Mis Profit Center],[Client Code].[Old Main Location],[Client Code].[Regional Head],[Client Code].[Relationship Manager Code],[Client Code].[Relationship Manager Name],[Client Code].[Servicing Branch],[Client Code].[Servicing City],[Client Code].[Servicing State],[Client Code].[Servicing Sublocation],[Client Code].[Servicing Zone],[Client Code].[Sourcing Branch],[Client Code].[Sourcing City],[Client Code].[Sourcing Company],[Client Code].[Sourcing Person Code],[Client Code].[Sourcing Person Name],[Client Code].[Sourcing State],[Client Code].[Sourcing Sublocation],[Client Code].[Sourcing Zone],[Client Code].[Vertical] on Rows From [Sales] where ([Date].[FiscalPeriod].[Fiscal Year].[FY 2017])\" \n",
    "\n",
    "mdx_query3 = \"Select {[Measures].[DP Valuation]} on Columns,{[Client Code].children} DIMENSION PROPERTIES MEMBER_CAPTION,[Client Code].[Client Name],[Client Code].[Introduction Month],[Client Code].[Dealer Code],[Client Code].[Dealer Name],[Client Code].[Location Code],[Client Code].[Mis Profit Center],[Client Code].[Sourcing Company],[Client Code].[Location-Type of Client] on Rows From [Sales] where ([Date].[FiscalPeriod].[Fiscal Year].[FY 2018])\" \n",
    "\n",
    "\n",
    "dataset2 = rs.Open(mdx_query3, conn, 0, 1)\n",
    "t2 = rs.GetRows()\n",
    "df2 = pd.DataFrame(list(t2))\n",
    "t_df2 = df2.transpose()\n",
    "t_df2.columns = ['client_cd','Location_Code','Mis_Profit_Center','Sourcing_Company','Location_Type_of_Client','DP_Valuation']\n",
    "\n",
    "\n",
    "#consider we have exixting data frame df1 which have client code(say column name Client_Code) and brokerage\n",
    "\n",
    "#we shall merge df1 and t_df2\n",
    "df1['key1'] = df1.Client_Code.str.strip()\n",
    "t_df2['key1'] = t_df2.client_cd.str.strip()\n",
    "df_merge1 = pd.merge(df1,t_df2, how='left', left_on=['key1'], right_on=['key1'])\n",
    "\n",
    "#say we also have account status(i.e. account is active/deactive/closed etc)data frame df3(say client code column name client_id )\n",
    "\n",
    "#we shall merge df_merge1 and df3\n",
    "df_merge1['key1'] = df1.Client_Code.str.strip()\n",
    "df3['key1'] = df3.client_id.str.strip()\n",
    "df_merge3 = pd.merge(df_merge1,df3, how='left', left_on=['key1'], right_on=['key1'])\n",
    "\n",
    "#Exclusion code for KSL #df_merge3 is dataframe name\n",
    "df_merge3['Location_Code_mid'] = df_merge3['Location Code'].str[2:4]\n",
    "df_merge3['Location_Code_beg'] = df_merge3['Location Code'].str[:4]\n",
    "df_merge3['Location_Code_end'] = df_merge3['Location Code'].str[8:10]\n",
    "\n",
    "\n",
    "df_merge3['Exclusion_type'] = \"NA\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_beg'] == \"IBQN\"] = \"Online NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_beg'] == \"IBQR\"] = \"Online NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"RE\"] = \"Offline NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"RO\"] = \"Offline NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"NR\"] = \"Offline NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"NO\"] = \"Offline NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location Code'] == \"DBDBDBDBPC\"] = \"Offline NRI\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"MG\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"GM\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"ST\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"TG\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"SG\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"QT\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"MR\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"SF\"] = \"STAFF Account\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location_Code_mid'] == \"WM\"] = \"Wealth\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Sourcing_Company'] == \"Wealth Management\"] = \"Wealth left\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Sourcing_Company'] == \"KS-SUB BROKER\"] = \"SUB BROKER\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Sourcing_Company'] == \"KS-DSA\"] = \"DSA\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Sourcing_Company'] == \"KS-AUTHORIZED PERSON\"] = \"AUTHORIZED PERSON\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Sourcing_Company'] == \"Kotak Bank-NR-Privy\"] = \"NR-PRIVY\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"Wealth\"] = \"Wealth left final\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"CATIII\"] = \"CATIII Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"CATII\"] = \"CATII Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"QFI\"] = \"QFI Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"PCG\"] = \"PCG Type Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"NRI\"] = \"NRI Type Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"Staff\"] = \"Staff Type Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Mis Profit Center'] == \"PROP\"] = \"PROP Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"PROP\"] = \"PROP_left Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Mis Profit Center'] == \"INSTI\"] = \"INSTI Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"INSTI\"] = \"INSTI_left Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Mis Profit Center'] == \"PMS\"] = \"PMS Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['Location-Type Of Client'] == \"PMS\"] = \"PMS_left Client\"\n",
    "#df_merge3['Exclusion_type'][df_merge3['Mis Profit Center'] == \"PCG\"] = \"PCG Client\"\n",
    "df_merge3['Exclusion_type'][df_merge3['b.status'] == \"SUSP\"] = \"Closed Account\"\n",
    "\n",
    "df_final1 = df_merge3[df_merge3['Exclusion_type'] == \"NA\"]#df_final1 is new dataframe name after all exclusion\n",
    "\n",
    "df_final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating New Columns\n",
    "\n",
    "#New column is a function of existing columns\n",
    "df[‘NewColumn1’]=df[‘column2’] #Create a copy of existing column2 \n",
    "df[‘NewColumn2’]=df[‘column2’]+10 #Add 10 to existing column2 then create a new one\n",
    "df[‘NewColumn3’]= df[‘column1’] + df[‘column2’] #Add elements of column1 and column2  then create new column\n",
    "\n",
    "#adding new data frame as a column to existing data frame\n",
    "df = df.assign(df1=df1.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python code of alternate of if function in excel #df1,df2,df3 all data frames\n",
    "import numpy as np\n",
    "df['NewColumn'] = np.where(df['column1'] > 0, df['column2']/df_all_merge2['column1'], 0)\n",
    "df1['NewColumn2'] = np.where(df1['column3'] > 0.5, 1, 0)\n",
    "df3['NewColumn3'] = np.where(df3['column4'] <= 0, 1, 0)\n",
    "df4['NewColumn4'] = np.where(df4['column5'] < df4['column6']*0.4, 1, 0)\n",
    "df5['NewColumn5'] = df5['column6'] + (df5['column7'] - df5['column8'])\n",
    "df6['NewColumn6'] = np.where(df6['column9'] < -0.2, 1, 0)\n",
    "df7['NewColumn7'] = np.where(df7['DP_NAME'].str.startswith('KOTAK'),1,0)\n",
    "df8['Client_Introduction_Date'] = df8['Client_Introduction_Date'].astype('str')\n",
    "\n",
    "#convert to numeric   #here intro_yr is column of df1\n",
    "df1.intro_yr = pd.to_numeric(df1.intro_yr, errors='coerce')\n",
    "\n",
    "\n",
    "#others examples\n",
    "#finding month on board from today\n",
    "df2['MOB'] = (2017 - df2['intro_yr']*1)*12 + (8-df2['intro_mt']*1)\n",
    "#explanation of above code:(current yr.-intro yr)*12+(current month number - intro month)#we have done *1 to convert into numeric \n",
    "df2['MOB'] = (df2['crnt_yr'] - df2['intro_yr'])*12 + (df2['crnt_mt']-df2['intro_mt'])\n",
    "\n",
    "#To get median\n",
    "df2['days_traded_median'] = df2[['Total_no_days_traded_1','Total_no_days_traded_2','Total_no_days_traded_3']].median(axis=1)\n",
    "\n",
    "##others examples\n",
    "df2['days_traded_Group'] = 9\n",
    "\n",
    "df2['days_traded_Group'][df2['days_traded_median'] >= 10] = 3\n",
    "df2['days_traded_Group'][(df2['days_traded_median'] >= 5) & (df2['days_traded_median'] < 10)] = 2\n",
    "df2['days_traded_Group'][(df2['days_traded_median'] >= 1) & (df2['days_traded_median'] < 5)] = 1\n",
    "                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Handling Missing Values\n",
    "\n",
    "# dropna: It is used to drop rows or columns having missing data\n",
    "df1.dropna()\n",
    "\n",
    "# fillna: It is used to fill missing values\n",
    "df2.fillna(value=5) #It replaces all missing values with 5\n",
    "mean=df2[‘column1’].mean()\n",
    "df2[‘column1’].fillna(mean) #It replaces all missing values of column1 with mean  of available values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorting   #'client_code','AS_ON_Date' are two columns\n",
    "df1_sorted = df1.sort_values(by=['client_code','AS_ON_Date'], ascending=[True,True])\n",
    "#If i want descending for 'AS_ON_Date' values\n",
    "df1_sorted = df1.sort_values(by=['client_code','AS_ON_Date'], ascending=[True,False])\n",
    "\n",
    "#indexing(for 15)\n",
    "df2 = df1_sorted.groupby('client_code').head(15).reset_index(drop=True)#here indexing will be from 0 to 14\n",
    "#if i want indexing from 1 to 15\n",
    "df2['idx'] = df2['AS_ON_Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#giving names to columns after indexing/transposing\n",
    "client_code= df1_sorted.groupby('client_code').head(1).reset_index(drop=True)\n",
    "client_code= client_code[['client_code']]\n",
    "client_code['idx'] = client_code.groupby('client_code').cumcount()\n",
    "client_code['name_idx'] = 'client_code'\n",
    "client_code = client_code.pivot(index='client_code',columns='name_idx',values='client_code')\n",
    "\n",
    "# cli_month1_15['idx'] = cli_month1_15.groupby('client_code').cumcount()\n",
    "\n",
    "df2['cashrealize_idx'] = 'cli_cashrealize_' + df2.idx.astype(str)\n",
    "df2['dervrealize_idx'] = 'cli_dervrealize_' + df2.idx.astype(str)\n",
    "# cli_month1_15['cashrunreal_idx'] = 'cli_cashrunreal_' + cli_month1_15.idx.astype(str)\n",
    "# cli_month1_15['dervrunreal_idx'] = 'cli_dervrunreal_' + cli_month1_15.idx.astype(str)\n",
    "\n",
    "Cashrealized = df2.pivot(index='client_code',columns='cashrealize_idx',values='cashrealize')\n",
    "Derivativerealized = df2.pivot(index='client_code',columns='dervrealize_idx',values='dervrealize')\n",
    "# Cashunrealized = cli_month1_15.pivot(index='client_code',columns='cashrunreal_idx',values='cashrunreal')\n",
    "# Derivativeunrealized = cli_month1_15.pivot(index='client_code',columns='dervrunreal_idx',values='dervrunreal')\n",
    "\n",
    "df3= pd.concat([client_code,Cashrealized,Derivativerealized],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sequencing\n",
    "sequence = ['client_code','cli_cashrealize_201604','cli_cashrealize_201605','cli_cashrealize_201606','cli_cashrealize_201607','cli_cashrealize_201608','cli_cashrealize_201609','cli_cashrealize_201610','cli_cashrealize_201611','cli_cashrealize_201612','cli_cashrealize_201701','cli_cashrealize_201702','cli_cashrealize_201703','cli_cashrealize_201704','cli_cashrealize_201705','cli_cashrealize_201706','cli_dervrealize_201604','cli_dervrealize_201605','cli_dervrealize_201606','cli_dervrealize_201607','cli_dervrealize_201608','cli_dervrealize_201609','cli_dervrealize_201610','cli_dervrealize_201611','cli_dervrealize_201612','cli_dervrealize_201701','cli_dervrealize_201702','cli_dervrealize_201703','cli_dervrealize_201704','cli_dervrealize_201705','cli_dervrealize_201706']\n",
    "df4 = df3.reindex(columns=sequence)\n",
    "df4.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aggregate\n",
    "\n",
    "#groupby\n",
    "df.groupby(‘column1’).sum()\n",
    "df.groupby([‘column1’,’column2’]).count()\n",
    "\n",
    "#cummulative count\n",
    "df.groupby('client_code').cumcount()\n",
    "\n",
    "#pivot\n",
    "df1= pd.pivot_table(df, index=[\"dpname\",\"commonscripcode\"], values=[\"Total_Realized_PNL\"],aggfunc=[sum])\n",
    "df2= pd.pivot_table(df, index=[\"dpname\",\"commonscripcode\"], values=[\"Total_Realized_PNL\"],aggfunc=[len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple cross tabulation of two factors\n",
    "pd.crosstab(df.column1, df.column2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to merge two tables\n",
    "df1['key1'] = df1.client_cd.str.strip()\n",
    "df2['key1'] = df2.client_cd.str.strip()\n",
    "df_merge = pd.merge(df1,df2, how='left', left_on=['key1'], right_on=['key1'])\n",
    "df_merge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to strip\n",
    "dataframe.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify missing values of dataframe\n",
    "df.isnull()\n",
    "#to fill NA values\n",
    "df.fillna(0, inplace=True)#here na values are replaced by 0\n",
    "\n",
    "#Example to impute missing values in Age by the mean\n",
    "import numpy as np\n",
    "#Using numpy mean function to calculate the mean value\n",
    "meanAge = np.mean(df.Age)\n",
    "#replacing missing values in the DataFrame\n",
    "df.Age = df.Age.fillna(meanAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to transpose data\n",
    "dataframe.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#append\n",
    "df3=df1.append(df2, ignore_index=False, verify_integrity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to concat\n",
    "df1 = pd.concat([df1,df2], axis=1)#side(row) wise concat\n",
    "df2 = pd.concat([df1, df2])# down wise(column) concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to check data type\n",
    "df.dtypes\n",
    "#to check data summary #mean,count,min,max\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#identify unique values\n",
    "df[‘Column1’].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to remove duplicate values of a variable\n",
    "#Remove Duplicate Values based on values of variables \"Gender\" and \"BMI\"\n",
    "\n",
    "df1=df.drop_duplicates(['Gender', 'BMI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying function to element, column or dataframe\n",
    "#a. Map: It iterates over each element of a series\n",
    "df[‘column1’].map(lambda x: 10+x) #this will add 10 to each element of column1\n",
    "\n",
    "df[‘column2’].map(lambda x: ‘AV’+x)#this will concatenate “AV“ at the beginning of each element of column2 (column format is string) \n",
    "\n",
    "#b. Apply: As the name suggests, applies a function along any axis of the DataFrame\n",
    "df[[‘column1’,’column2’]].apply(sum)#it will returns the sum of all the values of column1 and column2\n",
    "                  \n",
    "#c. ApplyMap: This helps to apply a function to each element of dataframe.\n",
    "func = lambda x: x+2\n",
    "df.applymap(func) #it will add 2 to each element of dataframe (all columns of dataframe must be numeric type)\n",
    "                \n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic Stats\n",
    "#covariance\n",
    "df.cov()\n",
    "\n",
    "#correlation\n",
    "df.corr()\n",
    "\n",
    "#How to create plots (Histogram, Scatter, Box Plot)\n",
    "#Histogram\n",
    "#Plot Histogram\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df=pd.read_excel(\"E:/First.xlsx\", \"Sheet1\")\n",
    "#Plots in matplotlib reside within a figure object, use plt.figure to create new figure\n",
    " \n",
    "fig=plt.figure()\n",
    "#Create one or more subplots using add_subplot, because you can't create blank figure\n",
    " \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "#Variable\n",
    "ax.hist(df['Age'],bins = 5)\n",
    "#Labels and Tit\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('#Employee')\n",
    "plt.show()\n",
    "\n",
    "#Scatter plot\n",
    "#Plots in matplotlib reside within a figure object, use plt.figure to create new figure\n",
    " \n",
    "fig=plt.figure()\n",
    "#Create one or more subplots using add_subplot, because you can't create blank figure\n",
    "  \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "#Variable\n",
    "ax.scatter(df['Age'],df['Sales'])\n",
    "#Labels and Tit\n",
    "plt.title('Sales and Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "#Box-plot:\n",
    "import seaborn as sns\n",
    "sns.boxplot(df['Age'])\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to generate frequency tables with pandas\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_excel(\"E:/First.xlsx\", \"Sheet1\") \n",
    "\n",
    "test= df.groupby(['Gender','BMI'])\n",
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert numeric variables to string variables and vice versa\n",
    "srting_outcome = str(numeric_input) #Converts numeric_input to string_outcome\n",
    "integer_outcome = int(string_input) #Converts string_input to integer_outcome\n",
    "float_outcome = float(string_input) #Converts string_input to integer_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random sampling in pandas\n",
    "target=df.sample(frac=0.8,random_state=200)\n",
    "control=df.drop(target.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to write or function\n",
    "df1 = df[(df['Latest'] == \"Buy\") | (df['Latest'] == \"Add\") | (df['Latest'] == \"Sell\") | (df['Latest'] == \"Reduce\")]\n",
    "\n",
    "#how to write and function\n",
    "df['days_traded_Group'][(df['days_traded_median'] >= 5) & (df['days_traded_median'] < 10)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#round off #here cashrealize is column name\n",
    "df.cashrealize =df.cashrealize .round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to replace a value by other # here we are replacing 0.0 by 0\n",
    "df.replace(to_replace=\"0.0\", value=\"0\", inplace=True, limit=None, regex=False, method='pad', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to convert a column to string/text #here cashrealize is column name\n",
    "df['cashrealize'] = df['cashrealize'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to take values(text & number) of different columns of a same customer in a single column\n",
    "reshape['concatenated'] = \" cashrealize:\" + reshape['cashrealize'].astype(str) + \" \\n cashunreal:\" + reshape['cashunreal'].astype(str)\\\n",
    "+ \" \\n dervrealize:\" + reshape['dervrealize'].astype(str)+ \" \\n dervunreal:\" + reshape['dervunreal'].astype(str)\n",
    "#reshape['concatenated'] = \"cashrealize:\" + reshape['cashrealize'].astype(str) + \" || cashrunreal:\" + reshape['cashrunreal'].astype(str)\\\n",
    "#+ \" || dervrealize:\" + reshape['dervrealize'].astype(str)+ \" || dervrunreal:\" + reshape['dervrunreal'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to drop a column/multiple column\n",
    "df1.drop(['LongRealizedPNL','LongRealizedQty','LongTermAvgCost','Longavgcost','Longunreal','ShortAvgCost','ShortRealizedDELPNL',\\\n",
    "        'ShortRealizedSQUPPNL','ShortRealizedSQUPQty','ShortTermAvgCost','Shortunreal','avgcost','commonscripcode','is_reconciled','sector'], axis=1, inplace=True)\n",
    "\n",
    "       \n",
    "df1.drop('ShortRealizedDELQty',axis=1, inplace=True)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pulling data from MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pulling scrip level data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql as py\n",
    "\n",
    "conn = py.connect(user=\"admin\",passwd=\"L!fe$5432\",host=\"127.0.0.1\",port=3306,database=\"mysql\")\n",
    "\n",
    "q4 = \"SELECT * from mysql.scripwise_pnl_jun17 where DATE(dataason) = '2017-06-21' and (LongURqty > 0 or ShortURqty > 0)\" \n",
    "#q4 = \"SELECT * from mysql.pnl_main_data where (AS_ON_Date > 201603 and AS_ON_Date < 201707)\"\n",
    "\n",
    "df = pd.read_sql(q4,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pulling scrip level data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql as py\n",
    "\n",
    "conn = py.connect(user=\"admin\",passwd=\"L!fe$5432\",host=\"127.0.0.1\",port=3306,database=\"mysql\")\n",
    "\n",
    "q1 = \"SELECT * from mysql.scripwise_pnl_apr16 where (LongRealizedQty > 0 or ShortRealizedDELQty > 0 or ShortRealizedSQUPQty > 0 )\"\n",
    "q2 = \"SELECT * from mysql.scripwise_pnl_may16 where (LongRealizedQty > 0 or ShortRealizedDELQty > 0 or ShortRealizedSQUPQty > 0 )\"\n",
    "\n",
    "df1 = pd.read_sql(q1,conn)\n",
    "df2 = pd.read_sql(q2,conn)\n",
    "\n",
    "dfNew = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dynamically pulling out last 15 months’ PnL when only starting month is specified(here data taken from APR'16-Jun'17)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql as py\n",
    "\n",
    "conn = py.connect(user=\"admin\",passwd=\"L!fe$5432\",host=\"127.0.0.1\",port=3306,database=\"mysql\")\n",
    "\n",
    "q4 = \"SELECT * from mysql.pnl_main_data where (AS_ON_Date > 201603 and AS_ON_Date < 201707)\"\n",
    "\n",
    "#q4 = \"SELECT * from mysql.pnl_main_data where (AS_ON_Date > 201603 and AS_ON_Date < 201707)limit 200\"\n",
    "\n",
    "df = pd.read_sql(q4,conn)\n",
    "\n",
    "#consider we have exixting data frame df1 which have client code(say column name Client_Code) and brokerage\n",
    "\n",
    "#we shall merge df1 and t_df2\n",
    "df['key1'] = df.Client_Code.str.strip()\n",
    "df1['key1'] = df1.client_code.str.strip()\n",
    "df_merge1 = pd.merge(df,df1, how='left', left_on=['key1'], right_on=['key1'])\n",
    "df_merge1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pulling trade level data\n",
    "\n",
    "import pandas as pd\n",
    "import pyodbc as py\n",
    "import pymysql\n",
    "conn = pymysql.connect(user=\"admin\",passwd=\"L!fe$5432\",host=\"127.0.0.1\",port=3306,database=\"mysql\")\n",
    "conn9 = py.connect('Driver={SQL Server};''Server=Db28.kotakseconline.com;''Port=1433;''DATABASE=QSS')\n",
    "\n",
    "# pull trade data for Cash\n",
    "q16 = \"SELECT * from QSS.dbo.ClientTrans where Substring(CONVERT(CHAR(8),ClTransDate),1,6) = '201706'\"\n",
    "df = pd.read_sql(q16,conn9)\n",
    "df.to_sql(con=conn, name='cashtrd_jun17', if_exists='replace', flavor='mysql')\n",
    "\n",
    "# pull trade data for Derivatives\n",
    "q16 = \"SELECT * from QSS.dbo.DRCLTRANS where Substring(CONVERT(CHAR(8),ClDate),1,6) = '201705'\"\n",
    "df = pd.read_sql(q16,conn9)\n",
    "df.to_sql(con=conn, name='dervtrd_may17', if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to read from a sheet of excel/csv\n",
    "data_file = pd.read_excel('path_to_file.xls', sheetname=\"sheet_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write different worksheets in same excel\n",
    "writer20 = pd.ExcelWriter('D:\\Analytics_Shared\\Aditya\\Knowledge Series\\session_one\\pandas_multiple_top20.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "t_df_bank.head(100).to_excel(writer20, sheet_name='Bank')\n",
    "t_df_branch.head(100).to_excel(writer20, sheet_name='Branch')\n",
    "t_df_pcg.head(100).to_excel(writer20, sheet_name='PCG')\n",
    "t_df_franchisee.head(100).to_excel(writer20, sheet_name='Franchisee')\n",
    "# t_df_digital.head(20).to_excel(writer20, sheet_name='Digital')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer20.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nested if in python\n",
    "df6['CashInvestor_F&O_Trader'] = np.where(df6['Total_Derv_Vol'] > 0.8*df6['Total_Vol_cash_derv'],\"F&O-Player\",np.where(df6['Total_Square_Vol'] > 0.5*df6['Total_Vol_cash_derv'],\\\n",
    "                                                                                                                         \"Trader\",\"Cash-Investor\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting only one column from one dataframe and put it to another dataframe\n",
    "df1=df_merge2[['dpname']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df1.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Indexing, Slicing and Subsetting DataFrames in Python\n",
    "# select the first, second and third rows from the surveys variable\n",
    "df[0:3]\n",
    "# select the first 5 rows (rows 0,1,2,3,4)\n",
    "df[:5]\n",
    "# select the last element in the list\n",
    "# (the slice starts at the last element,\n",
    "# and ends at the end of the list)\n",
    "df[-1:]\n",
    "\n",
    "# set the first three rows of data in the DataFrame to 0\n",
    "surveys_copy[0:3] = 0\n",
    "\n",
    "#Slicing Subsets of Rows and Columns in Python\n",
    "#To select a subset of rows AND columns from our DataFrame, we can use the iloc method. For example, we can select month, day and year (columns 2, 3 and 4 if we start counting at 1), like this:\n",
    "df.iloc[0:3, 1:4]\n",
    "\n",
    "#Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you ask for 0:3, you are actually telling python to start at index 0 and select rows 0, 1, 2 up to but not including 3\n",
    "#note:using iloc for position level and loc for index level\n",
    "\n",
    "#We can also select a specific data value according to the specific row and column location within the data frame using the iloc function: dat.iloc[row,column].\n",
    "df.iloc[2,6]\n",
    "\n",
    "#important link\n",
    "#https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding location/row number by giving a particular column value\n",
    "df.loc[df['Client_Code']=='9XGC0'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding all details by giving a particular column value(also works if the value comes multiple times)\n",
    "df1=df[df['Client_Code']=='9XFN4']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert string to numeric\n",
    "t_df1['Today_Month'] = t_df1['Today_Month'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#date time convert/today\n",
    "import pandas as pd\n",
    "import pyodbc as py\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta\n",
    "tdate = datetime.strftime(datetime.now(), '%Y/%m/%d')\n",
    "idate = datetime.strftime(datetime.now() - timedelta(45), '%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove missing rows\n",
    "df_merge1=df_merge1[df_merge1['Commonscriptcode'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only take selected columns from data frame\n",
    "df11 = df[['client_cd_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to do round\n",
    "df_merge1['Gain_percent_1'] = np.round(df_merge1['Gain_percent_1'], decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to concatenate\n",
    "df_merge1['Gain_percent_1'].astype(str) + \"%\"\n",
    "#e.g.df_merge1['Gain_percent_1'] = np.where(df_merge1[\"Gain_percent_1\"] > 99, \">99%\",df_merge1['Gain_percent_1'].astype(str) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql as py\n",
    "\n",
    "conn = py.connect(user=\"admin\",passwd=\"L!fe$5432\",host=\"127.0.0.1\",port=3306,database=\"kotakbi\")\n",
    "\n",
    "cli_month = \"SELECT * from kotakbi.pnl_main_data where (AS_ON_Date > 201701 and AS_ON_Date < 201802)\"\n",
    "#cli_month = \"SELECT * from mysql.pnl_main_data LIMIT 1000\" \n",
    "cli_month1 = pd.read_sql(cli_month,conn)\n",
    "\n",
    "del cli_month1['row_names']\n",
    "\n",
    "df1_short= pd.pivot_table(cli_month1, index=[\"client_code\"], values=(\"cashrealize\",\"dervrealize\",\"cashrunreal\",\"dervrunreal\"),aggfunc=sum)\n",
    "\n",
    "dpname = cli_month1.groupby('client_code').head(1).reset_index(drop=True)\n",
    "dpname = dpname[['client_code']]\n",
    "dpname['idx'] = dpname.groupby('client_code').cumcount()\n",
    "dpname['name_idx'] = 'client_code'\n",
    "dpname = dpname.pivot(index='client_code',columns='name_idx',values='client_code')\n",
    "\n",
    "\n",
    "reshape = pd.concat([dpname,df1_short],axis=1)\n",
    "\n",
    "reshape.columns = ['name_idx','Cashrealize','Cashunreal','Dervrealize','Dervunreal']\n",
    "\n",
    "reshape.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "reshape['Cashrealize'] = np.round(reshape['Cashrealize'], decimals=1)\n",
    "reshape['Cashunreal'] = np.round(reshape['Cashunreal'], decimals=1)\n",
    "reshape['Dervrealize'] = np.round(reshape['Dervrealize'], decimals=1)\n",
    "reshape['Dervunreal'] = np.round(reshape['Dervunreal'], decimals=1)\n",
    "\n",
    "# reshape.cashrealize =reshape.Cashrealize .round()\n",
    "# reshape.cashunreal = reshape.Cashunreal.round()\n",
    "# reshape.dervrealize = reshape.Dervrealize.round()\n",
    "# reshape.dervunreal = reshape.Dervunreal.round()\n",
    "# reshape['Cashrealize'] = reshape['Cashrealize'].astype(str)\n",
    "# reshape['Cashrealize'] = reshape['Cashrealize'].str.replace('.0', '')\n",
    "# reshape['Cashunreal'] = reshape['Cashunreal'].astype(str)\n",
    "# reshape['Cashunreal'] = reshape['Cashunreal'].str.replace('.0', '')\n",
    "# reshape['Dervrealize'] = reshape['Dervrealize'].astype(str)\n",
    "# reshape['Dervrealize'] = reshape['Dervrealize'].str.replace('.0', '')\n",
    "# reshape['Dervunreal'] = reshape['Dervunreal'].astype(str)\n",
    "# reshape['Dervunreal'] = reshape['Dervunreal'].str.replace('.0', '')\n",
    "\n",
    "\n",
    "# reshape.replace(to_replace=\".0\", value=\"\", inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "\n",
    "reshape['concatenated'] = \" Cashrealize:\" + reshape['Cashrealize'].astype(str) + \" \\n Cashunreal:\" + reshape['Cashunreal'].astype(str)\\\n",
    "+ \" \\n Dervrealize:\" + reshape['Dervrealize'].astype(str)+ \" \\n Dervunreal:\" + reshape['Dervunreal'].astype(str)\n",
    "#reshape['concatenated'] = \"cashrealize:\" + reshape['cashrealize'].astype(str) + \" || cashrunreal:\" + reshape['cashrunreal'].astype(str)\\\n",
    "#+ \" || dervrealize:\" + reshape['dervrealize'].astype(str)+ \" || dervrunreal:\" + reshape['dervrunreal'].astype(str)\n",
    "\n",
    "reshape.to_csv(\"E:\\Analytics_Shared\\Abhirup\\Abhirup\\Python output\\p&l Feb17-Jan18_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:06:45.622918Z",
     "start_time": "2020-12-18T16:06:43.649047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClientCode</th>\n",
       "      <th>Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0MHD|HG4N0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P000L|MVHU0|KML00|98NP0|8VIK1|8T9E6|8RYW0|8QI47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EWAK0|BM1X0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DYP48|BJ670</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RZJ00|RSZU0|RB8L0|R0C0R|E9777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ClientCode  Flag\n",
       "0                                      M0MHD|HG4N0     1\n",
       "1  P000L|MVHU0|KML00|98NP0|8VIK1|8T9E6|8RYW0|8QI47     2\n",
       "2                                      EWAK0|BM1X0     3\n",
       "3                                      DYP48|BJ670     4\n",
       "4                    RZJ00|RSZU0|RB8L0|R0C0R|E9777     5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Temp00=pd.DataFrame(pd.read_csv(r'E:\\Analytics_Shared\\Varsha\\Family_Account\\Test_Text.txt'))\n",
    "Temp00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:06:59.813700Z",
     "start_time": "2020-12-18T16:06:58.342296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flag   \n",
       "1     0    M0MHD\n",
       "      1    HG4N0\n",
       "2     0    P000L\n",
       "      1    MVHU0\n",
       "      2    KML00\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temp01=pd.DataFrame(Temp00.ClientCode.str.split('|').tolist(),index=Temp00.Flag).stack()\n",
    "Temp01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T16:07:14.906422Z",
     "start_time": "2020-12-18T16:07:14.886404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65452, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temp02=Temp01.reset_index([0,'Flag'])\n",
    "Temp02.columns=['Flag','ClientCode']\n",
    "Temp02.head()\n",
    "Temp02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
